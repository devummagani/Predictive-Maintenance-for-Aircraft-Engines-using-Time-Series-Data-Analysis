{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "31d9a4c9",
      "metadata": {
        "origin_pos": 0,
        "id": "31d9a4c9"
      },
      "source": [
        "# Using Jupyter Notebooks\n",
        ":label:`sec_jupyter`\n",
        "\n",
        "\n",
        "This section describes how to edit and run the code\n",
        "in each section of this book\n",
        "using the Jupyter Notebook. Make sure you have\n",
        "installed Jupyter and downloaded the\n",
        "code as described in\n",
        ":ref:`chap_installation`.\n",
        "If you want to know more about Jupyter see the excellent tutorial in\n",
        "their [documentation](https://jupyter.readthedocs.io/en/latest/).\n",
        "\n",
        "\n",
        "## Editing and Running the Code Locally\n",
        "\n",
        "Suppose that the local path of the book's code is `xx/yy/d2l-en/`. Use the shell to change the directory to this path (`cd xx/yy/d2l-en`) and run the command `jupyter notebook`. If your browser does not do this automatically, open http://localhost:8888 and you will see the interface of Jupyter and all the folders containing the code of the book, as shown in :numref:`fig_jupyter00`.\n",
        "\n",
        "![The folders containing the code of this book.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/jupyter00.png?raw=1)\n",
        ":width:`600px`\n",
        ":label:`fig_jupyter00`\n",
        "\n",
        "\n",
        "You can access the notebook files by clicking on the folder displayed on the webpage.\n",
        "They usually have the suffix \".ipynb\".\n",
        "For the sake of brevity, we create a temporary \"test.ipynb\" file.\n",
        "The content displayed after you click it is\n",
        "shown in :numref:`fig_jupyter01`.\n",
        "This notebook includes a markdown cell and a code cell. The content in the markdown cell includes \"This Is a Title\" and \"This is text.\".\n",
        "The code cell contains two lines of Python code.\n",
        "\n",
        "![Markdown and code cells in the \"text.ipynb\" file.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/jupyter01.png?raw=1)\n",
        ":width:`600px`\n",
        ":label:`fig_jupyter01`\n",
        "\n",
        "\n",
        "Double click on the markdown cell to enter edit mode.\n",
        "Add a new text string \"Hello world.\" at the end of the cell, as shown in :numref:`fig_jupyter02`.\n",
        "\n",
        "![Edit the markdown cell.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/jupyter02.png?raw=1)\n",
        ":width:`600px`\n",
        ":label:`fig_jupyter02`\n",
        "\n",
        "\n",
        "As demonstrated in :numref:`fig_jupyter03`,\n",
        "click \"Cell\" $\\rightarrow$ \"Run Cells\" in the menu bar to run the edited cell.\n",
        "\n",
        "![Run the cell.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/jupyter03.png?raw=1)\n",
        ":width:`600px`\n",
        ":label:`fig_jupyter03`\n",
        "\n",
        "After running, the markdown cell is shown in :numref:`fig_jupyter04`.\n",
        "\n",
        "![The markdown cell after running.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/jupyter04.png?raw=1)\n",
        ":width:`600px`\n",
        ":label:`fig_jupyter04`\n",
        "\n",
        "\n",
        "Next, click on the code cell. Multiply the elements by 2 after the last line of code, as shown in :numref:`fig_jupyter05`.\n",
        "\n",
        "![Edit the code cell.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/jupyter05.png?raw=1)\n",
        ":width:`600px`\n",
        ":label:`fig_jupyter05`\n",
        "\n",
        "\n",
        "You can also run the cell with a shortcut (\"Ctrl + Enter\" by default) and obtain the output result from :numref:`fig_jupyter06`.\n",
        "\n",
        "![Run the code cell to obtain the output.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/jupyter06.png?raw=1)\n",
        ":width:`600px`\n",
        ":label:`fig_jupyter06`\n",
        "\n",
        "\n",
        "When a notebook contains more cells, we can click \"Kernel\" $\\rightarrow$ \"Restart & Run All\" in the menu bar to run all the cells in the entire notebook. By clicking \"Help\" $\\rightarrow$ \"Edit Keyboard Shortcuts\" in the menu bar, you can edit the shortcuts according to your preferences.\n",
        "\n",
        "## Advanced Options\n",
        "\n",
        "Beyond local editing two things are quite important: editing the notebooks in the markdown format and running Jupyter remotely.\n",
        "The latter matters when we want to run the code on a faster server.\n",
        "The former matters since Jupyter's native ipynb format stores a lot of auxiliary data that is\n",
        "irrelevant to the content,\n",
        "mostly related to how and where the code is run.\n",
        "This is confusing for Git, making\n",
        "reviewing contributions very difficult.\n",
        "Fortunately there is an alternative---native editing in the markdown format.\n",
        "\n",
        "### Markdown Files in Jupyter\n",
        "\n",
        "If you wish to contribute to the content of this book, you need to modify the\n",
        "source file (md file, not ipynb file) on GitHub.\n",
        "Using the notedown plugin we\n",
        "can modify notebooks in the md format directly in Jupyter.\n",
        "\n",
        "\n",
        "First, install the notedown plugin, run the Jupyter Notebook, and load the plugin:\n",
        "\n",
        "```\n",
        "pip install d2l-notedown  # You may need to uninstall the original notedown.\n",
        "jupyter notebook --NotebookApp.contents_manager_class='notedown.NotedownContentsManager'\n",
        "```\n",
        "\n",
        "You may also turn on the notedown plugin by default whenever you run the Jupyter Notebook.\n",
        "First, generate a Jupyter Notebook configuration file (if it has already been generated, you can skip this step).\n",
        "\n",
        "```\n",
        "jupyter notebook --generate-config\n",
        "```\n",
        "\n",
        "Then, add the following line to the end of the Jupyter Notebook configuration file (for Linux or macOS, usually in the path `~/.jupyter/jupyter_notebook_config.py`):\n",
        "\n",
        "```\n",
        "c.NotebookApp.contents_manager_class = 'notedown.NotedownContentsManager'\n",
        "```\n",
        "\n",
        "After that, you only need to run the `jupyter notebook` command to turn on the notedown plugin by default.\n",
        "\n",
        "### Running Jupyter Notebooks on a Remote Server\n",
        "\n",
        "Sometimes, you may want to run Jupyter notebooks on a remote server and access it through a browser on your local computer. If Linux or macOS is installed on your local machine (Windows can also support this function through third-party software such as PuTTY), you can use port forwarding:\n",
        "\n",
        "```\n",
        "ssh myserver -L 8888:localhost:8888\n",
        "```\n",
        "\n",
        "The above string `myserver` is the address of the remote server.\n",
        "Then we can use http://localhost:8888 to access the remote server `myserver` that runs Jupyter notebooks. We will detail on how to run Jupyter notebooks on AWS instances\n",
        "later in this appendix.\n",
        "\n",
        "### Timing\n",
        "\n",
        "We can use the `ExecuteTime` plugin to time the execution of each code cell in Jupyter notebooks.\n",
        "Use the following commands to install the plugin:\n",
        "\n",
        "```\n",
        "pip install jupyter_contrib_nbextensions\n",
        "jupyter contrib nbextension install --user\n",
        "jupyter nbextension enable execute_time/ExecuteTime\n",
        "```\n",
        "\n",
        "## Summary\n",
        "\n",
        "* Using the Jupyter Notebook tool, we can edit, run, and contribute to each section of the book.\n",
        "* We can run Jupyter notebooks on remote servers using port forwarding.\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Edit and run the code in this book with the Jupyter Notebook on your local machine.\n",
        "1. Edit and run the code in this book with the Jupyter Notebook *remotely* via port forwarding.\n",
        "1. Compare the running time of the operations $\\mathbf{A}^\\top \\mathbf{B}$ and $\\mathbf{A} \\mathbf{B}$ for two square matrices in $\\mathbb{R}^{1024 \\times 1024}$. Which one is faster?\n",
        "\n",
        "\n",
        "[Discussions](https://discuss.d2l.ai/t/421)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Colab-ready script: Predictive maintenance (CMAPSS-like) using uploaded archive.zip\n",
        "- Detects /mnt/data/archive.zip, extracts files into ./data/\n",
        "- Auto-identifies train/test/RUL (truth) files using common name patterns (or substring match)\n",
        "- Preprocesses, creates sliding windows, trains an LSTM, evaluates, saves model+scaler\n",
        "\"\"\"\n",
        "# === CELL 0: (optional) install libs in Colab ===\n",
        "# Uncomment if you need to install\n",
        "# !pip install --upgrade --quiet tensorflow pandas scikit-learn matplotlib joblib\n",
        "\n",
        "# === CELL 1: imports ===\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import zipfile\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras import backend as K\n",
        "import joblib\n",
        "\n",
        "print('Python', sys.version)\n",
        "\n",
        "# === CELL 2: prepare data dir and extract archive.zip ===\n",
        "DATA_DIR = Path('data')\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "ARCHIVE_PATH = Path('/content/archive.zip')  # user-upload location you mentioned\n",
        "\n",
        "if ARCHIVE_PATH.exists():\n",
        "    print('Found archive at', ARCHIVE_PATH)\n",
        "    with zipfile.ZipFile(ARCHIVE_PATH, 'r') as z:\n",
        "        print('Archive contents:')\n",
        "        for name in z.namelist():\n",
        "            print(' -', name)\n",
        "        print('Extracting to ./data/ ...')\n",
        "        z.extractall(DATA_DIR)\n",
        "else:\n",
        "    print('No /mnt/data/archive.zip found. If you uploaded files elsewhere, move them into this notebook or upload archive.zip to /mnt/data and rerun.')\n",
        "\n",
        "print('\\nFiles under ./data/:')\n",
        "for f in sorted(DATA_DIR.rglob('*')):\n",
        "    print('-', f.relative_to(Path.cwd()))\n",
        "\n",
        "# === CELL 3: locate train/test/RUL files (flexible matching) ===\n",
        "possible_train = ['train_FD001.txt','train_FD002.txt','train_FD003.txt','train_FD004.txt','train.txt','train.csv']\n",
        "possible_test  = ['test_FD001.txt','test_FD002.txt','test_FD003.txt','test_FD004.txt','test.txt','test.csv']\n",
        "possible_rul   = ['RUL_FD001.txt','RUL_FD002.txt','RUL_FD003.txt','RUL_FD004.txt','RUL.txt','RUL.csv','truth.txt','truth.csv','RUL_FD.txt']\n",
        "\n",
        "train_path = None\n",
        "test_path = None\n",
        "rul_path = None\n",
        "\n",
        "# exact matches first\n",
        "for f in DATA_DIR.rglob('*'):\n",
        "    name = f.name\n",
        "    if name in possible_train and train_path is None:\n",
        "        train_path = f\n",
        "    if name in possible_test and test_path is None:\n",
        "        test_path = f\n",
        "    if name in possible_rul and rul_path is None:\n",
        "        rul_path = f\n",
        "\n",
        "# fallback: substring matches\n",
        "if not train_path:\n",
        "    for f in DATA_DIR.rglob('*'):\n",
        "        if 'train' in f.name.lower():\n",
        "            train_path = f\n",
        "            break\n",
        "if not test_path:\n",
        "    for f in DATA_DIR.rglob('*'):\n",
        "        if 'test' in f.name.lower():\n",
        "            test_path = f\n",
        "            break\n",
        "if not rul_path:\n",
        "    for f in DATA_DIR.rglob('*'):\n",
        "        if 'rul' in f.name.lower() or 'truth' in f.name.lower():\n",
        "            rul_path = f\n",
        "            break\n",
        "\n",
        "print('\\nDetected files:')\n",
        "print('train:', train_path)\n",
        "print('test :', test_path)\n",
        "print('rul  :', rul_path)\n",
        "\n",
        "if not (train_path and test_path and rul_path):\n",
        "    raise FileNotFoundError(\"Could not find train/test/RUL files automatically. Check ./data contents and re-run. Files found above.\")\n",
        "\n",
        "# copy to consistent names\n",
        "shutil.copy(train_path, DATA_DIR / 'train.txt')\n",
        "shutil.copy(test_path,  DATA_DIR / 'test.txt')\n",
        "shutil.copy(rul_path,   DATA_DIR / 'RUL.txt')\n",
        "train_path = DATA_DIR / 'train.txt'\n",
        "test_path  = DATA_DIR / 'test.txt'\n",
        "rul_path   = DATA_DIR / 'RUL.txt'\n",
        "print('Files copied to data/train.txt, data/test.txt, data/RUL.txt')\n",
        "\n",
        "# === CELL 4: load CMAPSS-like data (flexible delimiter) ===\n",
        "col_names = ['unit','cycle'] + [f'op_{i}' for i in range(1,4)] + [f'sensor_{i}' for i in range(1,22)]\n",
        "\n",
        "def load_file_try(path):\n",
        "    # try whitespace separated first, then comma\n",
        "    try:\n",
        "        return pd.read_csv(path, sep=r'\\s+', header=None, names=col_names)\n",
        "    except Exception:\n",
        "        return pd.read_csv(path, header=None, names=col_names)\n",
        "\n",
        "train_df = load_file_try(train_path)\n",
        "test_df  = load_file_try(test_path)\n",
        "# RUL/truth usually single column\n",
        "try:\n",
        "    rul_df = pd.read_csv(rul_path, sep=r'\\s+', header=None, names=['RUL'])\n",
        "except Exception:\n",
        "    rul_df = pd.read_csv(rul_path, header=None, names=['RUL'])\n",
        "\n",
        "print('Shapes -> train:', train_df.shape, 'test:', test_df.shape, 'RUL:', rul_df.shape)\n",
        "\n",
        "# === CELL 5: compute RUL for train/test ===\n",
        "def add_rul_train(df):\n",
        "    d = df.copy()\n",
        "    d['RUL'] = d.groupby('unit')['cycle'].transform('max') - d['cycle']\n",
        "    return d\n",
        "\n",
        "train_df = add_rul_train(train_df)\n",
        "\n",
        "# For test: RUL file gives remaining life at end-of-test; compute per-row RUL\n",
        "last_cycle = test_df.groupby('unit')['cycle'].max().reset_index()\n",
        "last_cycle.columns = ['unit','last_cycle']\n",
        "\n",
        "if len(rul_df) != len(last_cycle):\n",
        "    print('Warning: RUL length != number of unique units in test. Will attempt to align by order.')\n",
        "\n",
        "# take the first len(last_cycle) rows from rul_df\n",
        "last_cycle['future_RUL'] = rul_df['RUL'].values[:len(last_cycle)]\n",
        "last_cycle_dict = last_cycle.set_index('unit')['last_cycle'].to_dict()\n",
        "future_rul_dict = last_cycle.set_index('unit')['future_RUL'].to_dict()\n",
        "\n",
        "test_df = test_df.copy()\n",
        "test_df['RUL'] = test_df['unit'].map(last_cycle_dict) + test_df['unit'].map(future_rul_dict) - test_df['cycle']\n",
        "\n",
        "print('Added RUL columns to train and test.')\n",
        "\n",
        "# === CELL 6: quick EDA (optional) ===\n",
        "print('\\nTrain sample:')\n",
        "display(train_df.head())\n",
        "\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.hist(train_df['RUL'], bins=40)\n",
        "plt.title('Train RUL distribution')\n",
        "plt.xlabel('RUL')\n",
        "plt.show()\n",
        "\n",
        "# === CELL 7: feature selection and scaling ===\n",
        "all_features = [c for c in train_df.columns if c.startswith('sensor_') or c.startswith('op_')]\n",
        "var = train_df[all_features].var()\n",
        "keep_cols = var[var > 0.0].index.tolist()\n",
        "FEATURE_COLUMNS = keep_cols\n",
        "LABEL_COLUMN = 'RUL'\n",
        "\n",
        "print('Selected features:', FEATURE_COLUMNS)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_df[FEATURE_COLUMNS])\n",
        "train_df_scaled = train_df.copy()\n",
        "train_df_scaled[FEATURE_COLUMNS] = scaler.transform(train_df[FEATURE_COLUMNS])\n",
        "test_df_scaled  = test_df.copy()\n",
        "test_df_scaled[FEATURE_COLUMNS]  = scaler.transform(test_df[FEATURE_COLUMNS])\n",
        "\n",
        "# === CELL 8: create sequences (padding for short engines) ===\n",
        "def create_sequences(df, seq_len=30, features=FEATURE_COLUMNS, label_col='RUL'):\n",
        "    seqs = []\n",
        "    labels = []\n",
        "    for uid in sorted(df['unit'].unique()):\n",
        "        unit_data = df[df['unit']==uid].sort_values('cycle')\n",
        "        X = unit_data[features].values\n",
        "        y = unit_data[label_col].values\n",
        "        if len(X) < seq_len:\n",
        "            pad_len = seq_len - len(X)\n",
        "            pad = np.repeat(X[[0], :], pad_len, axis=0)\n",
        "            seqs.append(np.vstack([pad, X]))\n",
        "            labels.append(y[-1])\n",
        "        else:\n",
        "            for start in range(0, len(X) - seq_len + 1):\n",
        "                end = start + seq_len\n",
        "                seqs.append(X[start:end])\n",
        "                labels.append(y[end-1])\n",
        "    return np.array(seqs), np.array(labels)\n",
        "\n",
        "SEQ_LEN = 30\n",
        "X_all, y_all = create_sequences(train_df_scaled, seq_len=SEQ_LEN)\n",
        "print('Total train sequences:', X_all.shape)\n",
        "\n",
        "# === CELL 9: train/val split by unit (no leakage) ===\n",
        "unit_ids = sorted(train_df['unit'].unique())\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(unit_ids)\n",
        "train_units = unit_ids[:int(0.8*len(unit_ids))]\n",
        "val_units   = unit_ids[int(0.8*len(unit_ids)):]\n",
        "\n",
        "def sequences_for_units(df_scaled, units, seq_len=SEQ_LEN, features=FEATURE_COLUMNS):\n",
        "    rows = df_scaled[df_scaled['unit'].isin(units)]\n",
        "    return create_sequences(rows, seq_len=seq_len, features=features)\n",
        "\n",
        "X_train, y_train = sequences_for_units(train_df_scaled, train_units)\n",
        "X_val,   y_val   = sequences_for_units(train_df_scaled, val_units)\n",
        "X_test,  y_test  = create_sequences(test_df_scaled, seq_len=SEQ_LEN)\n",
        "\n",
        "print('Shapes:')\n",
        "print('X_train', X_train.shape, 'y_train', y_train.shape)\n",
        "print('X_val  ', X_val.shape,   'y_val  ', y_val.shape)\n",
        "print('X_test ', X_test.shape,  'y_test ', y_test.shape)\n",
        "\n",
        "# === CELL 10: build LSTM model ===\n",
        "K.clear_session()\n",
        "model = Sequential([\n",
        "    Masking(mask_value=0., input_shape=(SEQ_LEN, len(FEATURE_COLUMNS))),\n",
        "    LSTM(128, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(64),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse', metrics=[\"RootMeanSquaredError\"])\n",
        "model.summary()\n",
        "\n",
        "# callbacks and training\n",
        "checkpoint_path = 'best_model.h5'\n",
        "callbacks = [\n",
        "    ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1),\n",
        "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=60,\n",
        "    batch_size=64,\n",
        "    callbacks=callbacks,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# === CELL 11: evaluate, plot, save ===\n",
        "model.load_weights(checkpoint_path)\n",
        "\n",
        "train_pred = model.predict(X_train).ravel()\n",
        "val_pred   = model.predict(X_val).ravel()\n",
        "test_pred  = model.predict(X_test).ravel()\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return math.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "print('Train RMSE:', rmse(y_train, train_pred))\n",
        "print('Val RMSE:', rmse(y_val, val_pred))\n",
        "print('Test RMSE:', rmse(y_test, test_pred))\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.title('Training loss')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(y_test, test_pred, alpha=0.4)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--')\n",
        "plt.xlabel('Actual RUL')\n",
        "plt.ylabel('Predicted RUL')\n",
        "plt.title('Test: Actual vs Predicted RUL')\n",
        "plt.show()\n",
        "\n",
        "model.save('lstm_cmapss_model')\n",
        "joblib.dump(scaler, 'feature_scaler.pkl')\n",
        "print('Saved model -> lstm_cmapss_model and scaler -> feature_scaler.pkl')\n",
        "\n",
        "# === CELL 12: quick inference example ===\n",
        "if len(X_test) > 0:\n",
        "    idx = np.random.randint(0, len(X_test))\n",
        "    sample_seq = X_test[idx:idx+1]\n",
        "    true_rul = y_test[idx]\n",
        "    pred_rul = float(model.predict(sample_seq).ravel()[0])\n",
        "    print(f'Sample predicted RUL: {pred_rul:.2f} ; Actual RUL: {true_rul:.2f}')\n",
        "else:\n",
        "    print('No test sequences created; check test file parsing / seq len.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "gezcDxdlwnv3",
        "outputId": "5b535e84-60fc-4cff-eba8-93d6358b3244"
      },
      "id": "gezcDxdlwnv3",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "Found archive at /content/archive.zip\n",
            "Archive contents:\n",
            " - PM_test.csv\n",
            " - PM_train.csv\n",
            " - PM_truth.csv\n",
            "Extracting to ./data/ ...\n",
            "\n",
            "Files under ./data/:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "'data/PM_test.csv' is not in the subpath of '/content'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3419183433.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nFiles under ./data/:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelative_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# === CELL 3: locate train/test/RUL files (flexible matching) ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mrelative_to\u001b[0;34m(self, other, walk_up, *_deprecated)\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwalk_up\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{str(self)!r} is not in the subpath of {str(other)!r}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'..' segment in {str(other)!r} cannot be walked\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 'data/PM_test.csv' is not in the subpath of '/content'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Predictive Maintenance Project Code (Adjusted for '/content/archive.zip')\n",
        "\n",
        "\"\"\"\n",
        "Predictive maintenance using LSTM on aviation engine sensor data.\n",
        "Updated to use the uploaded file located at: '/content/archive.zip'\n",
        "This version loads PM_train.csv, PM_test.csv, PM_truth.csv directly after extraction.\n",
        "\"\"\"\n",
        "\n",
        "# ==========================\n",
        "# CELL 1 — Imports & Setup\n",
        "# ==========================\n",
        "import os\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# ==========================\n",
        "# CELL 2 — Unzip uploaded archive\n",
        "# ==========================\n",
        "UPLOAD_ZIP = \"/content/archive.zip\"  # fixed path\n",
        "DATA_DIR = Path(\"data\")\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Unzipping {UPLOAD_ZIP} ...\")\n",
        "with zipfile.ZipFile(UPLOAD_ZIP, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR)\n",
        "\n",
        "print(\"Extracted files:\")\n",
        "for f in DATA_DIR.iterdir():\n",
        "    print(\" -\", f)\n",
        "\n",
        "# ==========================\n",
        "# CELL 3 — Load CSV files\n",
        "# ==========================\n",
        "train_file = DATA_DIR / \"PM_train.csv\"\n",
        "test_file = DATA_DIR / \"PM_test.csv\"\n",
        "truth_file = DATA_DIR / \"PM_truth.csv\"\n",
        "\n",
        "train_df = pd.read_csv(train_file)\n",
        "test_df = pd.read_csv(test_file)\n",
        "truth_df = pd.read_csv(truth_file)\n",
        "\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Test shape:\", test_df.shape)\n",
        "print(\"Truth shape:\", truth_df.shape)\n",
        "\n",
        "# ==========================\n",
        "# CELL 4 — Create RUL for training\n",
        "# ==========================\n",
        "rul_train = train_df.groupby('unit_number')['time_in_cycles'].max().reset_index()\n",
        "rul_train.columns = ['unit_number', 'max_cycle']\n",
        "\n",
        "train_df = train_df.merge(rul_train, on='unit_number', how='left')\n",
        "train_df['RUL'] = train_df['max_cycle'] - train_df['time_in_cycles']\n",
        "train_df.drop('max_cycle', axis=1, inplace=True)\n",
        "\n",
        "# ==========================\n",
        "# CELL 5 — Normalization\n",
        "# ==========================\n",
        "FEATURES = train_df.columns.drop(['unit_number', 'time_in_cycles', 'RUL'])\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "train_df[FEATURES] = scaler.fit_transform(train_df[FEATURES])\n",
        "test_df[FEATURES] = scaler.transform(test_df[FEATURES])\n",
        "\n",
        "# ==========================\n",
        "# CELL 6 — Sequence creation\n",
        "# ==========================\n",
        "SEQ_LEN = 30\n",
        "\n",
        "def create_sequences(df, seq_len, target_col):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for engine_id in df['unit_number'].unique():\n",
        "        edata = df[df['unit_number']==engine_id].reset_index(drop=True)\n",
        "        matrix = edata[FEATURES].values\n",
        "\n",
        "        for i in range(len(edata) - seq_len):\n",
        "            sequences.append(matrix[i:i+seq_len])\n",
        "            targets.append(edata.loc[i+seq_len, target_col])\n",
        "\n",
        "    return np.array(sequences), np.array(targets)\n",
        "\n",
        "X_train, y_train = create_sequences(train_df, SEQ_LEN, \"RUL\")\n",
        "\n",
        "# ==========================\n",
        "# CELL 7 — Build LSTM model\n",
        "# ==========================\n",
        "model = Sequential([\n",
        "    LSTM(64, return_sequences=True, input_shape=(SEQ_LEN, len(FEATURES))),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32),\n",
        "    Dropout(0.2),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "model.summary()\n",
        "\n",
        "# ==========================\n",
        "# CELL 8 — Train model\n",
        "# ==========================\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ==========================\n",
        "# CELL 9 — Prepare test sequences\n",
        "# ==========================\n",
        "def last_sequence_per_engine(df, seq_len):\n",
        "    lst = []\n",
        "    for eid in df['unit_number'].unique():\n",
        "        sub = df[df['unit_number']==eid].reset_index(drop=True)\n",
        "        mat = sub[FEATURES].values\n",
        "        if len(mat) >= seq_len:\n",
        "            lst.append(mat[-seq_len:])\n",
        "    return np.array(lst)\n",
        "\n",
        "X_test = last_sequence_per_engine(test_df, SEQ_LEN)\n",
        "\n",
        "# ==========================\n",
        "# CELL 10 — Predict RUL\n",
        "# ==========================\n",
        "pred_rul = model.predict(X_test).flatten()\n",
        "print(\"Predicted RUL sample:\", pred_rul[:10])\n",
        "print(\"Actual RUL sample:\", truth_df['RUL'].values[:10])\n",
        "\n",
        "# ==========================\n",
        "# CELL 11 — Evaluation\n",
        "# ==========================\n",
        "rmse = np.sqrt(mean_squared_error(truth_df['RUL'], pred_rul))\n",
        "print(\"RMSE:\", rmse)\n",
        "\n",
        "# ==========================\n",
        "# CELL 12 — Save model\n",
        "# ==========================\n",
        "model.save(\"predictive_maintenance_lstm.h5\")\n",
        "print(\"Model saved: predictive_maintenance_lstm.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "T6NizK5sz2WP",
        "outputId": "c627027f-4129-44b5-be95-bad71f1245cf"
      },
      "id": "T6NizK5sz2WP",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "GPU available: []\n",
            "Unzipping /content/archive.zip ...\n",
            "Extracted files:\n",
            " - data/PM_train.csv\n",
            " - data/PM_truth.csv\n",
            " - data/PM_test.csv\n",
            "Train shape: (20631, 26)\n",
            "Test shape: (13096, 26)\n",
            "Truth shape: (100, 2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'unit_number'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3634241708.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# CELL 4 — Create RUL for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# ==========================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mrul_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unit_number'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time_in_cycles'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0mrul_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'unit_number'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max_cycle'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   9181\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to supply one of 'by' and 'level'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9183\u001b[0;31m         return DataFrameGroupBy(\n\u001b[0m\u001b[1;32m   9184\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9185\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgrouper\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1041\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'unit_number'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/data/PM_train.csv\")\n",
        "print(df.columns)\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEDrbNeS0ddX",
        "outputId": "5f6207a9-4151-4175-9555-0cf8733b125d"
      },
      "id": "JEDrbNeS0ddX",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
            "       's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
            "       's15', 's16', 's17', 's18', 's19', 's20', 's21'],\n",
            "      dtype='object')\n",
            "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n",
            "0   1      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70  1400.60   \n",
            "1   1      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82  1403.14   \n",
            "2   1      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99  1404.20   \n",
            "3   1      4    0.0007    0.0000     100.0  518.67  642.35  1582.79  1401.87   \n",
            "4   1      5   -0.0019   -0.0002     100.0  518.67  642.37  1582.85  1406.22   \n",
            "\n",
            "      s5  ...     s12      s13      s14     s15   s16  s17   s18    s19  \\\n",
            "0  14.62  ...  521.66  2388.02  8138.62  8.4195  0.03  392  2388  100.0   \n",
            "1  14.62  ...  522.28  2388.07  8131.49  8.4318  0.03  392  2388  100.0   \n",
            "2  14.62  ...  522.42  2388.03  8133.23  8.4178  0.03  390  2388  100.0   \n",
            "3  14.62  ...  522.86  2388.08  8133.83  8.3682  0.03  392  2388  100.0   \n",
            "4  14.62  ...  522.19  2388.04  8133.80  8.4294  0.03  393  2388  100.0   \n",
            "\n",
            "     s20      s21  \n",
            "0  39.06  23.4190  \n",
            "1  39.00  23.4236  \n",
            "2  38.95  23.3442  \n",
            "3  38.88  23.3739  \n",
            "4  38.90  23.4044  \n",
            "\n",
            "[5 rows x 26 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv(\"/content/data/PM_test.csv\")\n",
        "print(df_test.columns)\n",
        "print(df_test.head())\n",
        "\n",
        "df_truth = pd.read_csv(\"/content/data/PM_truth.csv\")\n",
        "print(df_truth.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C8z3-bV1QMz",
        "outputId": "c4b40eae-50b1-493f-cbd1-e2bd9a95d70b"
      },
      "id": "5C8z3-bV1QMz",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
            "       's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
            "       's15', 's16', 's17', 's18', 's19', 's20', 's21'],\n",
            "      dtype='object')\n",
            "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n",
            "0   1      1    0.0023    0.0003     100.0  518.67  643.02  1585.29  1398.21   \n",
            "1   1      2   -0.0027   -0.0003     100.0  518.67  641.71  1588.45  1395.42   \n",
            "2   1      3    0.0003    0.0001     100.0  518.67  642.46  1586.94  1401.34   \n",
            "3   1      4    0.0042    0.0000     100.0  518.67  642.44  1584.12  1406.42   \n",
            "4   1      5    0.0014    0.0000     100.0  518.67  642.51  1587.19  1401.92   \n",
            "\n",
            "      s5  ...     s12      s13      s14     s15   s16  s17   s18    s19  \\\n",
            "0  14.62  ...  521.72  2388.03  8125.55  8.4052  0.03  392  2388  100.0   \n",
            "1  14.62  ...  522.16  2388.06  8139.62  8.3803  0.03  393  2388  100.0   \n",
            "2  14.62  ...  521.97  2388.03  8130.10  8.4441  0.03  393  2388  100.0   \n",
            "3  14.62  ...  521.38  2388.05  8132.90  8.3917  0.03  391  2388  100.0   \n",
            "4  14.62  ...  522.15  2388.03  8129.54  8.4031  0.03  390  2388  100.0   \n",
            "\n",
            "     s20      s21  \n",
            "0  38.86  23.3735  \n",
            "1  39.02  23.3916  \n",
            "2  39.08  23.4166  \n",
            "3  39.00  23.3737  \n",
            "4  38.99  23.4130  \n",
            "\n",
            "[5 rows x 26 columns]\n",
            "   id  cycle\n",
            "0   1    112\n",
            "1   2     98\n",
            "2   3     69\n",
            "3   4     82\n",
            "4   5     91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YOzl4j505c_Z"
      },
      "id": "YOzl4j505c_Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Predictive Maintenance Project Code (Adjusted for '/content/archive.zip')\n",
        "\n",
        "\"\"\"\n",
        "Predictive maintenance using LSTM on aviation engine sensor data.\n",
        "Updated to use the uploaded file located at: '/content/archive.zip'\n",
        "This version loads PM_train.csv, PM_test.csv, PM_truth.csv directly after extraction.\n",
        "\"\"\"\n",
        "\n",
        "# ==========================\n",
        "# CELL 1 — Imports & Setup\n",
        "# ==========================\n",
        "import pandas as pd\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/data/PM_train.csv\")\n",
        "print(df.columns)\n",
        "print(df.head())\n",
        "df_test = pd.read_csv(\"/content/data/PM_test.csv\")\n",
        "print(df_test.columns)\n",
        "print(df_test.head())\n",
        "\n",
        "df_truth = pd.read_csv(\"/content/data/PM_truth.csv\")\n",
        "print(df_truth.head())\n",
        "import os\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# ==========================\n",
        "# CELL 2 — Unzip uploaded archive\n",
        "# ==========================\n",
        "UPLOAD_ZIP = \"/content/archive.zip\"  # fixed path\n",
        "DATA_DIR = Path(\"data\")\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Unzipping {UPLOAD_ZIP} ...\")\n",
        "with zipfile.ZipFile(UPLOAD_ZIP, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR)\n",
        "\n",
        "print(\"Extracted files:\")\n",
        "for f in DATA_DIR.iterdir():\n",
        "    print(\" -\", f)\n",
        "\n",
        "# ==========================\n",
        "# CELL 3 — Load CSV files\n",
        "# ==========================\n",
        "train_file = DATA_DIR / \"PM_train.csv\"\n",
        "test_file = DATA_DIR / \"PM_test.csv\"\n",
        "truth_file = DATA_DIR / \"PM_truth.csv\"\n",
        "\n",
        "train_df = pd.read_csv(train_file)\n",
        "test_df = pd.read_csv(test_file)\n",
        "truth_df = pd.read_csv(truth_file)\n",
        "\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Test shape:\", test_df.shape)\n",
        "print(\"Truth shape:\", truth_df.shape)\n",
        "\n",
        "# ==========================\n",
        "# CELL 4 — Create RUL for training\n",
        "# ==========================\n",
        "rul_train = train_df.groupby('id')['cycle'].max().reset_index()\n",
        "rul_train.columns = ['id', 'max_cycle']\n",
        "\n",
        "train_df = train_df.merge(rul_train, on='unit_number', how='left')\n",
        "train_df['RUL'] = train_df['max_cycle'] - train_df['cycles']\n",
        "train_df.drop('max_cycle', axis=1, inplace=True)\n",
        "\n",
        "# ==========================\n",
        "# CELL 5 — Normalization\n",
        "# ==========================\n",
        "FEATURES = train_df.columns.drop(['id', 'cycles', 'RUL'])\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "train_df[FEATURES] = scaler.fit_transform(train_df[FEATURES])\n",
        "test_df[FEATURES] = scaler.transform(test_df[FEATURES])\n",
        "\n",
        "# ==========================\n",
        "# CELL 6 — Sequence creation\n",
        "# ==========================\n",
        "SEQ_LEN = 30\n",
        "\n",
        "def create_sequences(df, seq_len, target_col):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for engine_id in df['id'].unique():\n",
        "        edata = df[df['id']==engine_id].reset_index(drop=True)\n",
        "        matrix = edata[FEATURES].values\n",
        "\n",
        "        for i in range(len(edata) - seq_len):\n",
        "            sequences.append(matrix[i:i+seq_len])\n",
        "            targets.append(edata.loc[i+seq_len, target_col])\n",
        "\n",
        "    return np.array(sequences), np.array(targets)\n",
        "\n",
        "X_train, y_train = create_sequences(train_df, SEQ_LEN, \"RUL\")\n",
        "\n",
        "# ==========================\n",
        "# CELL 7 — Build LSTM model\n",
        "# ==========================\n",
        "model = Sequential([\n",
        "    LSTM(64, return_sequences=True, input_shape=(SEQ_LEN, len(FEATURES))),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32),\n",
        "    Dropout(0.2),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "model.summary()\n",
        "\n",
        "# ==========================\n",
        "# CELL 8 — Train model\n",
        "# ==========================\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ==========================\n",
        "# CELL 9 — Prepare test sequences\n",
        "# ==========================\n",
        "def last_sequence_per_engine(df, seq_len):\n",
        "    lst = []\n",
        "    for eid in df['id'].unique():\n",
        "        sub = df[df['id']==eid].reset_index(drop=True)\n",
        "        mat = sub[FEATURES].values\n",
        "        if len(mat) >= seq_len:\n",
        "            lst.append(mat[-seq_len:])\n",
        "    return np.array(lst)\n",
        "\n",
        "X_test = last_sequence_per_engine(test_df, SEQ_LEN)\n",
        "\n",
        "# ==========================\n",
        "# CELL 10 — Predict RUL\n",
        "# ==========================\n",
        "pred_rul = model.predict(X_test).flatten()\n",
        "print(\"Predicted RUL sample:\", pred_rul[:10])\n",
        "print(\"Actual RUL sample:\", truth_df['RUL'].values[:10])\n",
        "\n",
        "# ==========================\n",
        "# CELL 11 — Evaluation\n",
        "# ==========================\n",
        "rmse = np.sqrt(mean_squared_error(truth_df['RUL'], pred_rul))\n",
        "print(\"RMSE:\", rmse)\n",
        "\n",
        "# ==========================\n",
        "# CELL 12 — Save model\n",
        "# ==========================\n",
        "model.save(\"predictive_maintenance_lstm.h5\")\n",
        "print(\"Model saved: predictive_maintenance_lstm.h5\")\n"
      ],
      "metadata": {
        "outputId": "ee30e002-6f9a-4999-f65e-a1fc7f9f3cba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L2WQFNdf58kD"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
            "       's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
            "       's15', 's16', 's17', 's18', 's19', 's20', 's21'],\n",
            "      dtype='object')\n",
            "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n",
            "0   1      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70  1400.60   \n",
            "1   1      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82  1403.14   \n",
            "2   1      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99  1404.20   \n",
            "3   1      4    0.0007    0.0000     100.0  518.67  642.35  1582.79  1401.87   \n",
            "4   1      5   -0.0019   -0.0002     100.0  518.67  642.37  1582.85  1406.22   \n",
            "\n",
            "      s5  ...     s12      s13      s14     s15   s16  s17   s18    s19  \\\n",
            "0  14.62  ...  521.66  2388.02  8138.62  8.4195  0.03  392  2388  100.0   \n",
            "1  14.62  ...  522.28  2388.07  8131.49  8.4318  0.03  392  2388  100.0   \n",
            "2  14.62  ...  522.42  2388.03  8133.23  8.4178  0.03  390  2388  100.0   \n",
            "3  14.62  ...  522.86  2388.08  8133.83  8.3682  0.03  392  2388  100.0   \n",
            "4  14.62  ...  522.19  2388.04  8133.80  8.4294  0.03  393  2388  100.0   \n",
            "\n",
            "     s20      s21  \n",
            "0  39.06  23.4190  \n",
            "1  39.00  23.4236  \n",
            "2  38.95  23.3442  \n",
            "3  38.88  23.3739  \n",
            "4  38.90  23.4044  \n",
            "\n",
            "[5 rows x 26 columns]\n",
            "Index(['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
            "       's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
            "       's15', 's16', 's17', 's18', 's19', 's20', 's21'],\n",
            "      dtype='object')\n",
            "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n",
            "0   1      1    0.0023    0.0003     100.0  518.67  643.02  1585.29  1398.21   \n",
            "1   1      2   -0.0027   -0.0003     100.0  518.67  641.71  1588.45  1395.42   \n",
            "2   1      3    0.0003    0.0001     100.0  518.67  642.46  1586.94  1401.34   \n",
            "3   1      4    0.0042    0.0000     100.0  518.67  642.44  1584.12  1406.42   \n",
            "4   1      5    0.0014    0.0000     100.0  518.67  642.51  1587.19  1401.92   \n",
            "\n",
            "      s5  ...     s12      s13      s14     s15   s16  s17   s18    s19  \\\n",
            "0  14.62  ...  521.72  2388.03  8125.55  8.4052  0.03  392  2388  100.0   \n",
            "1  14.62  ...  522.16  2388.06  8139.62  8.3803  0.03  393  2388  100.0   \n",
            "2  14.62  ...  521.97  2388.03  8130.10  8.4441  0.03  393  2388  100.0   \n",
            "3  14.62  ...  521.38  2388.05  8132.90  8.3917  0.03  391  2388  100.0   \n",
            "4  14.62  ...  522.15  2388.03  8129.54  8.4031  0.03  390  2388  100.0   \n",
            "\n",
            "     s20      s21  \n",
            "0  38.86  23.3735  \n",
            "1  39.02  23.3916  \n",
            "2  39.08  23.4166  \n",
            "3  39.00  23.3737  \n",
            "4  38.99  23.4130  \n",
            "\n",
            "[5 rows x 26 columns]\n",
            "   id  cycle\n",
            "0   1    112\n",
            "1   2     98\n",
            "2   3     69\n",
            "3   4     82\n",
            "4   5     91\n",
            "TensorFlow version: 2.19.0\n",
            "GPU available: []\n",
            "Unzipping /content/archive.zip ...\n",
            "Extracted files:\n",
            " - data/PM_train.csv\n",
            " - data/PM_truth.csv\n",
            " - data/PM_test.csv\n",
            "Train shape: (20631, 26)\n",
            "Test shape: (13096, 26)\n",
            "Truth shape: (100, 2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'unit_number'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1499634643.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mrul_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max_cycle'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrul_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unit_number'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RUL'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_cycle'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cycles'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'max_cycle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m  10830\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10832\u001b[0;31m         return merge(\n\u001b[0m\u001b[1;32m  10833\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10834\u001b[0m             \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         op = _MergeOperation(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1295\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'unit_number'"
          ]
        }
      ],
      "id": "L2WQFNdf58kD"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Predictive maintenance using LSTM on aviation engine sensor data.\n",
        "Assumes uploaded archive at '/content/archive.zip' and CSVs inside 'data/' after extraction:\n",
        " - PM_train.csv\n",
        " - PM_test.csv\n",
        " - PM_truth.csv\n",
        "\"\"\"\n",
        "\n",
        "# --------------------------\n",
        "# CELL 1 — Imports & Setup\n",
        "# --------------------------\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# --------------------------\n",
        "# CELL 2 — Unzip uploaded archive\n",
        "# --------------------------\n",
        "UPLOAD_ZIP = \"/content/archive.zip\"  # fixed path\n",
        "DATA_DIR = Path(\"data\")\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Unzipping {UPLOAD_ZIP} ...\")\n",
        "with zipfile.ZipFile(UPLOAD_ZIP, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR)\n",
        "\n",
        "print(\"Extracted files:\")\n",
        "for f in sorted(DATA_DIR.iterdir()):\n",
        "    print(\" -\", f.name)\n",
        "\n",
        "# --------------------------\n",
        "# CELL 3 — Load CSV files\n",
        "# --------------------------\n",
        "train_file = DATA_DIR / \"PM_train.csv\"\n",
        "test_file = DATA_DIR / \"PM_test.csv\"\n",
        "truth_file = DATA_DIR / \"PM_truth.csv\"\n",
        "\n",
        "train_df = pd.read_csv(train_file)\n",
        "test_df = pd.read_csv(test_file)\n",
        "truth_df = pd.read_csv(truth_file)\n",
        "\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Test shape:\", test_df.shape)\n",
        "print(\"Truth shape:\", truth_df.shape)\n",
        "print(\"Train columns:\", train_df.columns.tolist())\n",
        "\n",
        "# --------------------------\n",
        "# CELL 4 — Ensure consistent column names\n",
        "# --------------------------\n",
        "# Common dataset variants use 'id' and 'cycle' — adapt if your CSVs have other names.\n",
        "# If your files use different names (e.g. 'unit_number' or 'cycles'), change them here.\n",
        "# We'll standardize to 'id' and 'cycle'\n",
        "col_map = {}\n",
        "if 'unit_number' in train_df.columns:\n",
        "    col_map['unit_number'] = 'id'\n",
        "if 'cycles' in train_df.columns:\n",
        "    col_map['cycles'] = 'cycle'\n",
        "\n",
        "# Apply to all dataframes\n",
        "if col_map:\n",
        "    train_df = train_df.rename(columns=col_map)\n",
        "    test_df = test_df.rename(columns=col_map)\n",
        "\n",
        "# Confirm required columns exist\n",
        "required = {'id', 'cycle'}\n",
        "missing = required - set(train_df.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required columns in train_df: {missing}\")\n",
        "\n",
        "# --------------------------\n",
        "# CELL 5 — Create RUL for training\n",
        "# --------------------------\n",
        "# Compute max cycle per engine and then RUL = max_cycle - cycle\n",
        "rul_train = train_df.groupby('id')['cycle'].max().reset_index()\n",
        "rul_train.columns = ['id', 'max_cycle']\n",
        "\n",
        "train_df = train_df.merge(rul_train, on='id', how='left')\n",
        "train_df['RUL'] = train_df['max_cycle'] - train_df['cycle']\n",
        "train_df = train_df.drop(columns=['max_cycle'])\n",
        "\n",
        "# --------------------------\n",
        "# CELL 6 — Feature selection & Normalization\n",
        "# --------------------------\n",
        "# Remove identifier and target columns from features\n",
        "FEATURES = [c for c in train_df.columns if c not in ('id', 'cycle', 'RUL')]\n",
        "\n",
        "print(\"Using features:\", FEATURES)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "train_df[FEATURES] = scaler.fit_transform(train_df[FEATURES])\n",
        "# For test set, we must apply the same scaler. If test has same sensor columns, transform directly.\n",
        "test_df[FEATURES] = scaler.transform(test_df[FEATURES])\n",
        "\n",
        "# --------------------------\n",
        "# CELL 7 — Sequence creation\n",
        "# --------------------------\n",
        "SEQ_LEN = 30\n",
        "\n",
        "def create_sequences(df, seq_len, target_col, feature_cols):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    # iterate sorted ids to have deterministic order\n",
        "    for engine_id in sorted(df['id'].unique()):\n",
        "        edata = df[df['id'] == engine_id].sort_values('cycle').reset_index(drop=True)\n",
        "        matrix = edata[feature_cols].values\n",
        "        # create sliding windows; target is the RUL at the timestep after the sequence\n",
        "        for i in range(len(edata) - seq_len):\n",
        "            sequences.append(matrix[i:i+seq_len])\n",
        "            targets.append(edata.loc[i+seq_len, target_col])\n",
        "    return np.array(sequences), np.array(targets)\n",
        "\n",
        "X_train, y_train = create_sequences(train_df, SEQ_LEN, \"RUL\", FEATURES)\n",
        "print(\"X_train shape:\", X_train.shape, \"y_train shape:\", y_train.shape)\n",
        "\n",
        "# --------------------------\n",
        "# CELL 8 — Build LSTM model\n",
        "# --------------------------\n",
        "model = Sequential([\n",
        "    LSTM(64, return_sequences=True, input_shape=(SEQ_LEN, len(FEATURES))),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32),\n",
        "    Dropout(0.2),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "model.summary()\n",
        "\n",
        "# --------------------------\n",
        "# CELL 9 — Train model\n",
        "# --------------------------\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# CELL 10 — Prepare test sequences (last sequence per engine)\n",
        "# --------------------------\n",
        "def last_sequence_per_engine(df, seq_len, feature_cols):\n",
        "    lst = []\n",
        "    ids = []\n",
        "    for eid in sorted(df['id'].unique()):  # sorted to align with truth_df order (assumption)\n",
        "        sub = df[df['id'] == eid].sort_values('cycle').reset_index(drop=True)\n",
        "        mat = sub[feature_cols].values\n",
        "        if len(mat) >= seq_len:\n",
        "            lst.append(mat[-seq_len:])\n",
        "            ids.append(eid)\n",
        "        else:\n",
        "            # If not enough cycles to form a full seq, you could pad or skip; we skip here.\n",
        "            print(f\"Engine {eid} has {len(mat)} cycles (<{seq_len}) — skipping.\")\n",
        "    return np.array(lst), ids\n",
        "\n",
        "X_test, test_ids = last_sequence_per_engine(test_df, SEQ_LEN, FEATURES)\n",
        "print(\"X_test shape:\", X_test.shape, \"number of test engines used:\", len(test_ids))\n",
        "\n",
        "# --------------------------\n",
        "# CELL 11 — Predict RUL\n",
        "# --------------------------\n",
        "pred_rul = model.predict(X_test).flatten()\n",
        "print(\"Predicted RUL sample:\", pred_rul[:10])\n",
        "\n",
        "# --------------------------\n",
        "# CELL 12 — Align predictions with truth and evaluate\n",
        "# --------------------------\n",
        "# PM_truth usually contains RULs per engine in the same order as engine id 1..N.\n",
        "# We'll create a mapping from engine id to truth RUL.\n",
        "# First, ensure truth_df has one column with RUL (commonly single column).\n",
        "truth_df = truth_df.reset_index(drop=True)\n",
        "# If truth_df has more than one column, take the first numeric column as RUL\n",
        "if truth_df.shape[1] > 1:\n",
        "    truth_col = truth_df.columns[0]\n",
        "else:\n",
        "    truth_col = truth_df.columns[0]\n",
        "\n",
        "# Make dict mapping id -> truth RUL. If truth_df doesn't have ids, assume ordering matches sorted ids.\n",
        "if 'id' in truth_df.columns:\n",
        "    truth_map = dict(zip(truth_df['id'], truth_df[truth_col]))\n",
        "else:\n",
        "    # assume truth rows correspond to sorted engine ids in test set\n",
        "    sorted_test_ids = sorted(test_df['id'].unique())\n",
        "    truth_map = dict(zip(sorted_test_ids, truth_df[truth_col].values))\n",
        "\n",
        "# Build ground-truth array aligned with test_ids used\n",
        "truth_aligned = np.array([truth_map.get(eid, np.nan) for eid in test_ids])\n",
        "\n",
        "# Filter out any NaN (in case of missing)\n",
        "valid_mask = ~np.isnan(truth_aligned)\n",
        "if valid_mask.sum() == 0:\n",
        "    raise ValueError(\"No valid truth values found for the test engines used for prediction.\")\n",
        "\n",
        "pred_rul_aligned = pred_rul[valid_mask]\n",
        "truth_aligned = truth_aligned[valid_mask]\n",
        "\n",
        "print(\"Actual RUL sample:\", truth_aligned[:10])\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(truth_aligned, pred_rul_aligned))\n",
        "print(\"RMSE:\", rmse)\n",
        "\n",
        "# --------------------------\n",
        "# CELL 13 — Save model\n",
        "# --------------------------\n",
        "model.save(\"predictive_maintenance_lstm.h5\")\n",
        "print(\"Model saved: predictive_maintenance_lstm.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hAxK_Agz-YAu",
        "outputId": "8bbc50ee-5464-423e-e5bb-9230994573eb"
      },
      "id": "hAxK_Agz-YAu",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "GPU available: []\n",
            "Unzipping /content/archive.zip ...\n",
            "Extracted files:\n",
            " - PM_test.csv\n",
            " - PM_train.csv\n",
            " - PM_truth.csv\n",
            "Train shape: (20631, 26)\n",
            "Test shape: (13096, 26)\n",
            "Truth shape: (100, 2)\n",
            "Train columns: ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
            "Using features: ['setting1', 'setting2', 'setting3', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
            "X_train shape: (17631, 30, 24) y_train shape: (17631,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m22,784\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">22,784</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m35,745\u001b[0m (139.63 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">35,745</span> (139.63 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m35,745\u001b[0m (139.63 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">35,745</span> (139.63 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 28ms/step - loss: 9189.4131 - val_loss: 6678.9951\n",
            "Epoch 2/20\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - loss: 3759.9985 - val_loss: 5442.7827\n",
            "Epoch 3/20\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - loss: 3410.0107 - val_loss: 5380.9341\n",
            "Epoch 4/20\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 68ms/step - loss: 3535.7532 - val_loss: 5439.9990\n",
            "Epoch 5/20\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - loss: 3393.4722 - val_loss: 3577.2395\n",
            "Epoch 6/20\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - loss: 1553.3059 - val_loss: 2378.3474\n",
            "Epoch 7/20\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - loss: 1077.8826 - val_loss: 1794.4165\n",
            "Epoch 8/20\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - loss: 1010.6374 - val_loss: 1900.4244\n",
            "Epoch 9/20\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - loss: 960.1935 - val_loss: 1692.4148\n",
            "Epoch 10/20\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 27ms/step - loss: 929.6782 - val_loss: 1765.6799\n",
            "Epoch 11/20\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 32ms/step - loss: 894.5960 - val_loss: 1649.6130\n",
            "Epoch 12/20\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - loss: 938.2037 - val_loss: 1674.5077\n",
            "Epoch 13/20\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - loss: 887.9724 - val_loss: 1803.8953\n",
            "Epoch 14/20\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - loss: 853.7645 - val_loss: 1993.5244\n",
            "Epoch 15/20\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 27ms/step - loss: 856.1790 - val_loss: 1719.0465\n",
            "Epoch 16/20\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - loss: 892.1082 - val_loss: 1735.9146\n",
            "Epoch 17/20\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - loss: 844.7440 - val_loss: 1489.3940\n",
            "Epoch 18/20\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - loss: 854.9428 - val_loss: 1648.3889\n",
            "Epoch 19/20\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - loss: 862.5579 - val_loss: 1762.1783\n",
            "Epoch 20/20\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - loss: 865.9328 - val_loss: 1492.3846\n",
            "X_test shape: (100, 30, 24) number of test engines used: 100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted RUL sample: [130.39886  156.76396   57.90264   91.904854 123.12611  131.69188\n",
            " 111.208595 114.26902  154.96301   89.896   ]\n",
            "Actual RUL sample: [ 1  2  3  4  5  6  7  8  9 10]\n",
            "RMSE: 73.79513978745484\n",
            "Model saved: predictive_maintenance_lstm.h5\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}