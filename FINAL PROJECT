"""
Predictive maintenance using LSTM on aviation engine sensor data.
Assumes uploaded archive at '/content/archive.zip' and CSVs inside 'data/' after extraction:
 - PM_train.csv
 - PM_test.csv
 - PM_truth.csv
"""

# --------------------------
# CELL 1 — Imports & Setup
# --------------------------
import zipfile
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

print("TensorFlow version:", tf.__version__)
print("GPU available:", tf.config.list_physical_devices('GPU'))

# --------------------------
# CELL 2 — Unzip uploaded archive
# --------------------------
UPLOAD_ZIP = "/content/archive.zip"  # fixed path
DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)

print(f"Unzipping {UPLOAD_ZIP} ...")
with zipfile.ZipFile(UPLOAD_ZIP, "r") as zip_ref:
    zip_ref.extractall(DATA_DIR)

print("Extracted files:")
for f in sorted(DATA_DIR.iterdir()):
    print(" -", f.name)

# --------------------------
# CELL 3 — Load CSV files
# --------------------------
train_file = DATA_DIR / "PM_train.csv"
test_file = DATA_DIR / "PM_test.csv"
truth_file = DATA_DIR / "PM_truth.csv"

train_df = pd.read_csv(train_file)
test_df = pd.read_csv(test_file)
truth_df = pd.read_csv(truth_file)

print("Train shape:", train_df.shape)
print("Test shape:", test_df.shape)
print("Truth shape:", truth_df.shape)
print("Train columns:", train_df.columns.tolist())

# --------------------------
# CELL 4 — Ensure consistent column names
# --------------------------
# Common dataset variants use 'id' and 'cycle' — adapt if your CSVs have other names.
# If your files use different names (e.g. 'unit_number' or 'cycles'), change them here.
# We'll standardize to 'id' and 'cycle'
col_map = {}
if 'unit_number' in train_df.columns:
    col_map['unit_number'] = 'id'
if 'cycles' in train_df.columns:
    col_map['cycles'] = 'cycle'

# Apply to all dataframes
if col_map:
    train_df = train_df.rename(columns=col_map)
    test_df = test_df.rename(columns=col_map)

# Confirm required columns exist
required = {'id', 'cycle'}
missing = required - set(train_df.columns)
if missing:
    raise ValueError(f"Missing required columns in train_df: {missing}")

# --------------------------
# CELL 5 — Create RUL for training
# --------------------------
# Compute max cycle per engine and then RUL = max_cycle - cycle
rul_train = train_df.groupby('id')['cycle'].max().reset_index()
rul_train.columns = ['id', 'max_cycle']

train_df = train_df.merge(rul_train, on='id', how='left')
train_df['RUL'] = train_df['max_cycle'] - train_df['cycle']
train_df = train_df.drop(columns=['max_cycle'])

# --------------------------
# CELL 6 — Feature selection & Normalization
# --------------------------
# Remove identifier and target columns from features
FEATURES = [c for c in train_df.columns if c not in ('id', 'cycle', 'RUL')]

print("Using features:", FEATURES)

scaler = MinMaxScaler()
train_df[FEATURES] = scaler.fit_transform(train_df[FEATURES])
# For test set, we must apply the same scaler. If test has same sensor columns, transform directly.
test_df[FEATURES] = scaler.transform(test_df[FEATURES])

# --------------------------
# CELL 7 — Sequence creation
# --------------------------
SEQ_LEN = 30

def create_sequences(df, seq_len, target_col, feature_cols):
    sequences = []
    targets = []
    # iterate sorted ids to have deterministic order
    for engine_id in sorted(df['id'].unique()):
        edata = df[df['id'] == engine_id].sort_values('cycle').reset_index(drop=True)
        matrix = edata[feature_cols].values
        # create sliding windows; target is the RUL at the timestep after the sequence
        for i in range(len(edata) - seq_len):
            sequences.append(matrix[i:i+seq_len])
            targets.append(edata.loc[i+seq_len, target_col])
    return np.array(sequences), np.array(targets)

X_train, y_train = create_sequences(train_df, SEQ_LEN, "RUL", FEATURES)
print("X_train shape:", X_train.shape, "y_train shape:", y_train.shape)

# --------------------------
# CELL 8 — Build LSTM model
# --------------------------
model = Sequential([
    LSTM(64, return_sequences=True, input_shape=(SEQ_LEN, len(FEATURES))),
    Dropout(0.2),
    LSTM(32),
    Dropout(0.2),
    Dense(16, activation='relu'),
    Dense(1)
])

model.compile(optimizer="adam", loss="mse")
model.summary()

# --------------------------
# CELL 9 — Train model
# --------------------------
history = model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=20,
    batch_size=64,
    verbose=1
)

# --------------------------
# CELL 10 — Prepare test sequences (last sequence per engine)
# --------------------------
def last_sequence_per_engine(df, seq_len, feature_cols):
    lst = []
    ids = []
    for eid in sorted(df['id'].unique()):  # sorted to align with truth_df order (assumption)
        sub = df[df['id'] == eid].sort_values('cycle').reset_index(drop=True)
        mat = sub[feature_cols].values
        if len(mat) >= seq_len:
            lst.append(mat[-seq_len:])
            ids.append(eid)
        else:
            # If not enough cycles to form a full seq, you could pad or skip; we skip here.
            print(f"Engine {eid} has {len(mat)} cycles (<{seq_len}) — skipping.")
    return np.array(lst), ids

X_test, test_ids = last_sequence_per_engine(test_df, SEQ_LEN, FEATURES)
print("X_test shape:", X_test.shape, "number of test engines used:", len(test_ids))

# --------------------------
# CELL 11 — Predict RUL
# --------------------------
pred_rul = model.predict(X_test).flatten()
print("Predicted RUL sample:", pred_rul[:10])

# --------------------------
# CELL 12 — Align predictions with truth and evaluate
# --------------------------
# PM_truth usually contains RULs per engine in the same order as engine id 1..N.
# We'll create a mapping from engine id to truth RUL.
# First, ensure truth_df has one column with RUL (commonly single column).
truth_df = truth_df.reset_index(drop=True)
# If truth_df has more than one column, take the first numeric column as RUL
if truth_df.shape[1] > 1:
    truth_col = truth_df.columns[0]
else:
    truth_col = truth_df.columns[0]

# Make dict mapping id -> truth RUL. If truth_df doesn't have ids, assume ordering matches sorted ids.
if 'id' in truth_df.columns:
    truth_map = dict(zip(truth_df['id'], truth_df[truth_col]))
else:
    # assume truth rows correspond to sorted engine ids in test set
    sorted_test_ids = sorted(test_df['id'].unique())
    truth_map = dict(zip(sorted_test_ids, truth_df[truth_col].values))

# Build ground-truth array aligned with test_ids used
truth_aligned = np.array([truth_map.get(eid, np.nan) for eid in test_ids])

# Filter out any NaN (in case of missing)
valid_mask = ~np.isnan(truth_aligned)
if valid_mask.sum() == 0:
    raise ValueError("No valid truth values found for the test engines used for prediction.")

pred_rul_aligned = pred_rul[valid_mask]
truth_aligned = truth_aligned[valid_mask]

print("Actual RUL sample:", truth_aligned[:10])

rmse = np.sqrt(mean_squared_error(truth_aligned, pred_rul_aligned))
print("RMSE:", rmse)

# --------------------------
# CELL 13 — Save model
# --------------------------
model.save("predictive_maintenance_lstm.h5")
print("Model saved: predictive_maintenance_lstm.h5")
